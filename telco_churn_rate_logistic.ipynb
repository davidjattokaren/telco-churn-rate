{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a075bdc",
   "metadata": {},
   "source": [
    "# Data Mining Core Activities\n",
    "\n",
    "Before we begin solving any business problem that requires modelling of data, it is paramount that we complete the below checklist items (as part of a standard process regardless of the problem statement or/and domain):\n",
    "\n",
    "- [x] Business Requirements Gathering / Problem Identification\n",
    "- [x] Pre-requisites setup (Importing packages, etc.)\n",
    "- [x] Data source identification and access\n",
    "- [x] Exploratory Data Analysis\n",
    "- [x] Data Preparation\n",
    "- [x] Model Building & Evaluation\n",
    "- [x] Model Deployment\n",
    "\n",
    "While the activities above are not exhaustive, they cover the core requirements of any data modelling exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdd6505",
   "metadata": {},
   "source": [
    "# Business Requirements Gathering / Problem Identification\n",
    "\n",
    "We first appreciate the provided brief, and discuss the root problem at hand.\n",
    "\n",
    "- **The Brief:** \n",
    "\n",
    "    A telephone company X would like to take a proactive approach to managing their customer churn rate. After conducting in-depth surveys and stakeholder interviews, they have identified 5 variables that they feel would be most indicative of whether a customer will churn or not. They have approached us to help them build a solution that can allow the relevant stakeholders to input certain values (pertaining to a customer) of these 5 variables to arrive at a conclusion of whether the customer is likely to churn or not.\n",
    "\n",
    "\n",
    "- **Some Background on the Problem Statement:** \n",
    "\n",
    "    As a telephone company, there are a multitude of business problems that can be solved. However, it is important to prioritize them with some order so that one solves for those problems that have the maximum impact to the business. Some research was conducted on our side to validate the strength of the business problem:\n",
    "\n",
    "\n",
    "- <b>An Interesting Article: </b> [McKinsey - Reducing Churn in Telecom using Advanced Analytics](https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/reducing-churn-in-telecom-through-advanced-analytics)\n",
    "---\n",
    "- <u><b> Importance of Customer Retention (avoidance of Churn) illustrated: </b></u>\n",
    "\n",
    "    <img src =\"https://www.retentionscience.com/wp-content/uploads/2016/05/Retention-Science-Infographic.png\" alt = \"\" width = \"500\">\n",
    "\n",
    "    <img src =\"https://th.bing.com/th/id/R.8e388b8472d6c5265af63d793b02c4b2?rik=cFXewm9vrd%2bSpg&riu=http%3a%2f%2fthumbnails-visually.netdna-ssl.com%2fmobile-telco-churn-managing-fickle-customer-loyalty_51ba3dd1bfdac_w1500.jpg&ehk=w9erpgpBhbgpuWiEo1ZVqb42OUmFtmHC4Y%2bR3hE5Zgw%3d&risl=&pid=ImgRaw&r=0\" alt = \"\" width = \"600\"/>\n",
    "---\n",
    "\n",
    "\n",
    "- **Solution Requirement:** \n",
    "\n",
    "    The primary requirement is to provide a binary answer to the business:\n",
    "    - YES, the customer will churn, or \n",
    "    - NO, the customer will not churn\n",
    "    \n",
    "  To enable the business to test various combinations of customer profiles to arrive at this answer, we would need to build a predictive model. \n",
    "    \n",
    "  Considering that the expected output variable (churn) would have only 2 values (Yes / No), and that we need to build a predictive model, we can begin by choosing a Logistic Regression Model, as it fits the purpose of this problem statement.\n",
    "\n",
    "\n",
    "\n",
    "- **A few questions to think about before we get started:**\n",
    "\n",
    "    - How would the results of the model be actioned upon?\n",
    "    - What is the business impact of an incorrect output classification? Would we lose money (cost of retaining customer) or would the results be used in tandem with other inputs / outputs?\n",
    "    - Where is the business within the analytics lifecycle? Are the stakeholders ready to accept the output (and associated risks)?\n",
    "    - What level of training may need to be given to the users of this model, to ensure they know exactly how to use it and how to interpret the results?\n",
    "    - How do we demarcate what the model CAN and CANNOT answer? (to ensure there are no unrealistic expectations)\n",
    "\n",
    "  These questions would set the direction of future experiments on this predictive model.\n",
    "\n",
    "\n",
    "\n",
    "- **We are provided with 5 variables, namely:**\n",
    "\n",
    "    - *Churn*: Indicates if the customer (a row) churned or not (Yes (1) / No (0))\n",
    "    - *Senior Citizen*: Is the customer a senior citizen? (Yes (1) / No (0))\n",
    "    - *Dependents*: Does the customer have dependents? (Yes (1) / No (0))\n",
    "    - *Months with Company*: How many months has the customer been with the company? (No. of months)\n",
    "    - *Internet Service Status*: Does the customer have internet service with the company? (Yes (1) / No (0))\n",
    "    - *Month to Month Contract Status*: Does the customer have a month to month contract with the company? (Yes (1) / No (0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42341794",
   "metadata": {},
   "source": [
    "# Importing Required Packages\n",
    "\n",
    "**NOTE:** It is important to ensure that all required packages have been installed and upgraded BEFORE running the rest of this code. Without all pre-requisites being fulfilled, the code is likely to run into an error.\n",
    "\n",
    "Important lines of code to remember (all can be run on Anaconda Prompt):\n",
    "\n",
    "- pip install \"package_name\" (to install a package)\n",
    "- pip install --upgrade \"package_name\" (to upgrade a package)\n",
    "- python -m streamlit run app.py (for the first-time run of streamlit to ensure that we can bypass the Email input screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f22e60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import all required packages for the purpose of executing this model\n",
    "# Pandas helps us initialize and use the provided telco data in the form of a dataframe\n",
    "# Sklearn contains a set of linear models available, from which we select logistic regression (for the purpose of this model)\n",
    "# Sklearn also contains a set of metrics that can be used to evaluate the performance of our model. We chose accuracy score.\n",
    "# Pickle can serialize Python objects, and in this case will help us to save our model. This will allow for recreating / reusing the model when required.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36466af1",
   "metadata": {},
   "source": [
    "# Data Source Identification & Access\n",
    "\n",
    "Here, we connect to the data source that contains the information we need to begin solving this business problem. In this particular case, the data has been provided to us via a CSV file.\n",
    "\n",
    "<b>Questions</b>\n",
    "\n",
    "- Is the data provided a sample, or does it reflect the entire population of customers?\n",
    "- Can we connect to the database to pull data, instead of using a CSV file that takes additional time to generate and download?\n",
    "- What problems can occur when reading a comma separate values file? For e.g., what happens if there are commas present as values in any of the columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09790431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the telco dataset into a dataframe (using pandas). read_csv is from pandas and is used to read CSVs.\n",
    "df = pd.read_csv('telco_customer_churn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce83bc36",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "After storing the data into a dataframe, the first step is to analyze it to understand:\n",
    "\n",
    "- What does the data represent? \n",
    "- What might be the target variable? (If not already specified by the business)\n",
    "- What steps do we need to take to get the data ready for modeling?\n",
    "- What are the right features that we need to choose for the purpose of predicting the target variable?\n",
    "\n",
    "**Steps:**\n",
    "- [x] Data Exploration\n",
    "- [x] Data Cleaning\n",
    "- [x] Data Deep Dive - Numeric (Summary Stats, Outlier Treatment, Feature Transformation, Feature Selection)\n",
    "- [x] Data Deep Dive - Categorical (Summary Stats, Outlier Treatment, Feature Transformation, Feature Selection)\n",
    "- [x] Selection of Train & Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26562d0",
   "metadata": {},
   "source": [
    "><h3>Data Exploration </h3>\n",
    "First, we load the data and look at a few sample rows (first 5, last 5). We note down any data observations that may require a further deep-dive. Next, we summarize the data to gain more understanding of each column -  we check data types, number of NULLs, and summary statistics (such as mean, median, standard deviation, etc.). We may also try to identify a potential target variable (if not already specified by the business) at this point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0414c27",
   "metadata": {},
   "source": [
    "><h3>Data Cleaning </h3>\n",
    "Once we have a general understanding of the data, next we focus on improving the quality. The first step we can take is to drop any column that does not have a relationship with the target variable. Next, we identify columns having missing values and take appropriate remedial steps - dropping the column, filling the values using a simple or complex (such as clustering) methodology. For the purpose of building the predictive model it is important that we do not have a dataset that contains missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bc93c6",
   "metadata": {},
   "source": [
    "><h3>Data Deep Dive - Numeric</h3>\n",
    "Here, we first convert mis-classified data into numeric to ensure that all numeric columns have been correctly identified as numeric by pandas. Next, we check the summary statistics of data for any indications of outliers. We additionally plot a histogram, boxplot, and scatterplot as well to confirm the findings and identify any relationships between the target and any of the independent variables. Once the columns with outliers are established, we deep dive into them to understand the number of outlier rows, and the appropriate treatment method. Once this is complete and our numeric data is ready, we create a correlation matrix to identify correlated features and redundant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7b37b2",
   "metadata": {},
   "source": [
    "><h3>Data Deep Dive - Categorical</h3>\n",
    "For categorical variables, the first thing we want to understand is - what are the unique values for each categorical column? This will help to explain each column to understand it's effect on the target variable. Once this is complete, we can begin to summarize the target variable as a function of each unique value in the categorical column/s. If we observe that there is not much variance in the target variable (mean) between unique values of the categorical column, we may then choose to drop the column as it is not beneficial. We then convert the categorical variables to numerical indicators through a process known as \"one-hot encoding\". We ensure to drop those columns that are not beneficial in predicting / influencing the target variable value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8350834d",
   "metadata": {},
   "source": [
    "><h3>Selection of Train & Test data</h3>\n",
    "Once we have completed all of the aforementioned steps, we can create the testing and training dataset by choosing a random sample of 80% of the data for training purposes, while the remaining 20% is used for testing the model output to determine accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c6dd02",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "Now that we have completed the Exploratory Data Analysis and have prepared the columns in the data to be ready for modeling - we can now begin to set up the features and target variable for inclusion in the model function.\n",
    "\n",
    "<b>Questions</b>\n",
    "\n",
    "- Are the provided data points in the CSV the only available features, or are there others?\n",
    "- The below data prep stage would fail if the target variable occupies a position other than the first (index = 0). Can we select the variables based on the column name instead?\n",
    "    - What should we do if the column names also change?\n",
    "\n",
    "<b>Points to Note</b>\n",
    "\n",
    "- The below code works mainly for the scenario where:\n",
    "    - Churn is the target variable\n",
    "    - Churn is the first column in the dataset'\n",
    "    \n",
    "    We must think about how to make the code more scalable / dynamic to handle a case where any of the above 2 changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27be0c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X is the set of independent variables (features) that we will use in our model. iloc is used to find dataframe elements using integer indexes.\n",
    "X = df.iloc[:,1:len(df.columns)]\n",
    "\n",
    "#Y is the dependent variable (target variable, in this case \"churn\"). We have these values available in our data, and this will be used to train the model.\n",
    "y = df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f29bd0",
   "metadata": {},
   "source": [
    "# Model Building & Evaluation\n",
    "\n",
    "<b>Questions</b>\n",
    "\n",
    "- What do we do when we have new data points (new customers) coming in? How do we automate the model in such a way that it can take the latest data as an input?\n",
    "- What is an acceptable accuracy %? Would this be defined by us (data scientists), or by the business? (would the business have a say in what the accuracy % should be?)\n",
    "- How often should this model be run / updated? After what time period would the results start to become less reliable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d242cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7762317194377396\n"
     ]
    }
   ],
   "source": [
    "#We initialize a Logistic Regression model. Max_Iter is the maximum number of iterations the model is allowed to run before it can reach a solution.\n",
    "model = LogisticRegression(max_iter=800)\n",
    "\n",
    "#We train our model using the features in X and the results available in Y. This enables the model to understand data patterns that can be inferred between the features and the target variable.\n",
    "model.fit(X,y)\n",
    "\n",
    "#We now use the trained model to predict the target variable by providing the same set of features to it.\n",
    "predictions = model.predict(X)\n",
    "\n",
    "#The accuracy score is calculated by comparing the ACTUAL target variable values (as provided in the dataset) vs. the PREDICTED target variable values.\n",
    "print(accuracy_score(y,predictions))\n",
    "\n",
    "#We have achieved an accuracy score of 0.776, which is equal to 77.6%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb1328d",
   "metadata": {},
   "source": [
    "# Model Deployment\n",
    "\n",
    "<b>Questions</b>\n",
    "- How can we identify areas where performance of Streamlit can be optimized?\n",
    "- How do we deploy our Streamlit app onto client systems?\n",
    "- Is Streamlit the most optimal solution across various infrastructure types?\n",
    "- Is Streamlit secure? How can we ensure that only authorized stakeholders can access the app?\n",
    "- How can we test the Streamlit app to ensure that it doesn't \"break\" due to unexpected inputs, etc.?\n",
    "- What is more ideal - running the model on our end (\"heavy lifting\") and pushing just the results to client systems? Or running the end-to-end process on client systems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42f4e006",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create a file named \"classifier\", and write to it using \"wb\" mode (this writes the data in binary mode, ensuring that no changes are made to the data (for e.g., text encoding when writing out with text mode))\n",
    "pickle_out = open('classifier', mode='wb')\n",
    "\n",
    "#The model is \"dumped\" (stored) into the pickle_out file that was created in the previous line.\n",
    "pickle.dump(model, pickle_out)\n",
    "\n",
    "#The file that we just created is closed.\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff22aebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import pickle\n",
    "import streamlit as st\n",
    "\n",
    "pickle_in = open('classifier', 'rb')\n",
    "classifier = pickle.load(pickle_in)\n",
    "\n",
    "@st.cache()\n",
    "\n",
    "# Define the function which will make the prediction using data\n",
    "# inputs from users\n",
    "def prediction(senior_citizen, has_dependents,\n",
    "               months_as_customer, has_internet_service, has_month_to_month_contract):\n",
    "    \n",
    "    # Make predictions\n",
    "    prediction = classifier.predict(\n",
    "        [[senior_citizen, has_dependents, months_as_customer, has_internet_service, has_month_to_month_contract]])\n",
    "    \n",
    "    if prediction == 0:\n",
    "        pred = 'Customer is most likely NOT CHURNING'\n",
    "    else:\n",
    "        pred = 'Customer is most likely CHURNING'\n",
    "    return pred\n",
    "\n",
    "# This is the main function in which we define our webpage\n",
    "def main():\n",
    "    \n",
    "    st.title(\"Telco Customer Churn Predictor\")\n",
    "    st.write(\"Please enter the Customer profile values below:\")\n",
    "    col1, col2 = st.columns(2)\n",
    "    # Create input fields\n",
    "    with col1:\n",
    "        st.subheader(\"Personal Parameters\")\n",
    "        senior_citizen = st.number_input(\"Senior Citizen?\",\n",
    "                                  min_value=0,\n",
    "                                  max_value=1,\n",
    "                                  value=0,\n",
    "                                  step=1,\n",
    "                                 )\n",
    "        has_dependents = st.number_input(\"Has Dependents?\",\n",
    "                              min_value=0,\n",
    "                              max_value=1,\n",
    "                              value=0,\n",
    "                              step=1\n",
    "                             )\n",
    "\n",
    "        \n",
    "    with col2:\n",
    "        st.subheader(\"Relationship Parameters\")\n",
    "        months_as_customer = st.number_input(\"Months as Customer?\",\n",
    "                              min_value=0,\n",
    "                              #max_value=850,\n",
    "                              value=12,\n",
    "                              step=1\n",
    "                             )\n",
    "        has_internet_service = st.number_input(\"Has Internet Service?\",\n",
    "                          min_value=0,\n",
    "                          max_value=1,\n",
    "                          value=1,\n",
    "                          step=1\n",
    "                         )\n",
    "        has_month_to_month_contract = st.number_input(\"Has Month to Month Contract?\",\n",
    "                          min_value=0,\n",
    "                          max_value=1,\n",
    "                          value=1,\n",
    "                          step=1\n",
    "                         )\n",
    "\n",
    "    result = \"\"\n",
    "    \n",
    "    # When 'Predict' is clicked, make the prediction and store it\n",
    "    if st.button(\"ㅤㅤㅤPredictㅤㅤㅤ\"):\n",
    "        result = prediction(senior_citizen, has_dependents, months_as_customer, has_internet_service, has_month_to_month_contract)\n",
    "        if result == \"Customer is most likely NOT CHURNING\":\n",
    "            st.success(result)\n",
    "        else:\n",
    "            st.error(result)\n",
    "        \n",
    "    st.image(\"https://upload.wikimedia.org/wikipedia/commons/f/f3/W._P._Carey_School_of_Business_logo.png\", width = 150)\n",
    "    st.write(\"Built by David Joseph Attokaren\")\n",
    "    st.write(\"Version 1.0.0\")\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5522e88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run app.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "dea2a42eb633aff47016a198d741b1b015980498276dada8736771ac1bc27d8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
